{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a467eb0a-f8e2-4d2e-a6c9-5b83a57caab0",
   "metadata": {},
   "source": [
    "# Computing a Loss Landscape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cb32b4-2f67-4f9d-bebe-06ab3a31431f",
   "metadata": {},
   "source": [
    "This notebook demonstrates how you can use PyLossLandscapes to compute a loss landscape and save the resulting data to be analyzed later. We will use an RNN to classify names by their ethnic origin (from https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23393de9-68bc-4945-9604-2928f4810172",
   "metadata": {},
   "source": [
    "## Building and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf6d619-edb4-4b7b-bc82-19d660b37816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data.zip', <http.client.HTTPMessage at 0x7f13245eb750>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we have to download the dataset\n",
    "import urllib.request\n",
    "import subprocess\n",
    "urllib.request.urlretrieve(\"https://download.pytorch.org/tutorial/data.zip\", \"data.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "188dd6b9-5751-41b6-b6cf-164aa9648295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data.zip\n",
      "   creating: data/\n",
      "  inflating: data/eng-fra.txt        \n",
      "   creating: data/names/\n",
      "  inflating: data/names/Arabic.txt   \n",
      "  inflating: data/names/Chinese.txt  \n",
      "  inflating: data/names/Czech.txt    \n",
      "  inflating: data/names/Dutch.txt    \n",
      "  inflating: data/names/English.txt  \n",
      "  inflating: data/names/French.txt   \n",
      "  inflating: data/names/German.txt   \n",
      "  inflating: data/names/Greek.txt    \n",
      "  inflating: data/names/Irish.txt    \n",
      "  inflating: data/names/Italian.txt  \n",
      "  inflating: data/names/Japanese.txt  \n",
      "  inflating: data/names/Korean.txt   \n",
      "  inflating: data/names/Polish.txt   \n",
      "  inflating: data/names/Portuguese.txt  \n",
      "  inflating: data/names/Russian.txt  \n",
      "  inflating: data/names/Scottish.txt  \n",
      "  inflating: data/names/Spanish.txt  \n",
      "  inflating: data/names/Vietnamese.txt  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['unzip', 'data.zip'], returncode=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unzip the archive\n",
    "subprocess.run([\"unzip\", \"data.zip\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a4104f0-1bad-454f-a4ef-de020f868ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "torch.set_default_device(device)\n",
    "print(f\"Using device = {torch.get_default_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f886f36-6f4b-448d-a502-ecad134cf3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import unicodedata\n",
    "\n",
    "# We can use \"_\" to represent an out-of-vocabulary character, that is, any character we are not handling in our model\n",
    "allowed_characters = string.ascii_letters + \" .,;'\" + \"_\"\n",
    "n_letters = len(allowed_characters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in allowed_characters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9efa51bd-c774-4139-bda4-eda76b4747fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    # return our out-of-vocabulary character if we encounter a letter unknown to our model\n",
    "    if letter not in allowed_characters:\n",
    "        return allowed_characters.find(\"_\")\n",
    "    else:\n",
    "        return allowed_characters.find(letter)\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51ba11e1-d99b-4659-a374-518f43063c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NamesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir #for provenance of the dataset\n",
    "        self.load_time = time.localtime #for provenance of the dataset\n",
    "        labels_set = set() #set of all classes\n",
    "\n",
    "        self.data = []\n",
    "        self.data_tensors = []\n",
    "        self.labels = []\n",
    "        self.labels_tensors = []\n",
    "\n",
    "        #read all the ``.txt`` files in the specified directory\n",
    "        text_files = glob.glob(os.path.join(data_dir, '*.txt'))\n",
    "        for filename in text_files:\n",
    "            label = os.path.splitext(os.path.basename(filename))[0]\n",
    "            labels_set.add(label)\n",
    "            lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "            for name in lines:\n",
    "                self.data.append(name)\n",
    "                self.data_tensors.append(lineToTensor(name))\n",
    "                self.labels.append(label)\n",
    "\n",
    "        #Cache the tensor representation of the labels\n",
    "        self.labels_uniq = list(labels_set)\n",
    "        for idx in range(len(self.labels)):\n",
    "            temp_tensor = torch.tensor([self.labels_uniq.index(self.labels[idx])], dtype=torch.long)\n",
    "            self.labels_tensors.append(temp_tensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_item = self.data[idx]\n",
    "        data_label = self.labels[idx]\n",
    "        data_tensor = self.data_tensors[idx]\n",
    "        label_tensor = self.labels_tensors[idx]\n",
    "\n",
    "        return label_tensor, data_tensor, data_label, data_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f51f530-9c0c-45f5-b5f4-b766d4fffb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = NamesDataset(\"data/names\")\n",
    "#dataloader = DataLoader(alldata, batch_size=64, shuffle=False, num_workers=1, generator=torch.Generator(device='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bdc5eec-48e0-4bae-b7d6-dbb1c3a6e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = torch.utils.data.random_split(alldata, [.85, .15], generator=torch.Generator(device=device).manual_seed(2024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "052f3b19-9f11-4c8c-a014-4ad13086dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharRNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(input_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, line_tensor):\n",
    "        rnn_out, hidden = self.rnn(line_tensor)\n",
    "        output = self.h2o(hidden[0])\n",
    "        output = self.softmax(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63a2f015-db07-49dc-b4bc-878c7c609b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 128\n",
    "rnn = CharRNN(n_letters, n_hidden, len(alldata.labels_uniq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd423bad-b5e2-476d-a22e-ff1263559c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_from_output(output, output_labels):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    label_i = top_i[0].item()\n",
    "    return output_labels[label_i], label_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f37ff66-6351-4a0e-a020-4418e9adc431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_grad(model):\n",
    "    \"\"\"\n",
    "    get model parameters and corresponding gradients\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    grads = []\n",
    "    for param in model.parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        params.append(param)\n",
    "        grads.append(torch.tensor(0.0) if param.grad is None else param.grad + 0.)\n",
    "    return params, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2acc9a28-8867-4bee-b42f-ad465e4507bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def train(rnn, training_data, n_epoch = 10, n_batch_size = 64, report_every = 50, learning_rate = 0.2, criterion = nn.NLLLoss()):\n",
    "    \"\"\"\n",
    "    Learn on a batch of training_data for a specified number of iterations and reporting thresholds\n",
    "    \"\"\"\n",
    "    # Keep track of losses for plotting\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    rnn.train()\n",
    "    optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "    start = time.time()\n",
    "    print(f\"training on data set with n = {len(training_data)}\")\n",
    "\n",
    "    for iter in range(1, n_epoch + 1):\n",
    "        rnn.zero_grad() # clear the gradients\n",
    "\n",
    "        # create some minibatches\n",
    "        # we cannot use dataloaders because each of our names is a different length\n",
    "        batches = list(range(len(training_data)))\n",
    "        random.shuffle(batches)\n",
    "        batches = np.array_split(batches, len(batches) //n_batch_size )\n",
    "\n",
    "        for idx, batch in enumerate(batches):\n",
    "            batch_loss = 0\n",
    "            for i in batch: #for each example in this batch\n",
    "                (label_tensor, text_tensor, label, text) = training_data[i]\n",
    "                text_tensor = lineToTensor(text)\n",
    "                output = rnn.forward(text_tensor)\n",
    "                loss = criterion(output, label_tensor)\n",
    "                batch_loss += loss\n",
    "\n",
    "            # optimize parameters\n",
    "            batch_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(rnn.parameters(), 3)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            current_loss += batch_loss.item() / len(batch)\n",
    "\n",
    "        all_losses.append(current_loss / len(batches) )\n",
    "        if iter % report_every == 0:\n",
    "            print(f\"{iter} ({iter / n_epoch:.0%}): \\t average batch loss = {all_losses[-1]}\")\n",
    "        current_loss = 0\n",
    "\n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c83d9cf4-a99e-4602-b8cb-69831e5b12fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on data set with n = 17063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frosty/Programs/Python/pylosslandscapes/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:824: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /pytorch/torch/csrc/autograd/engine.cpp:1273.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 (19%): \t average batch loss = 0.8830110874460088\n",
      "10 (37%): \t average batch loss = 0.6981757688446498\n",
      "15 (56%): \t average batch loss = 0.5898481362210043\n",
      "20 (74%): \t average batch loss = 0.5048869946974768\n",
      "25 (93%): \t average batch loss = 0.45126905144726076\n"
     ]
    }
   ],
   "source": [
    "all_losses = train(rnn, train_set, n_epoch=27, learning_rate=0.15, report_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6b82402-fa48-48e5-9da1-3ad04c928ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(rnn.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89e716ab-e110-40ce-a326-f5da097cfb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = CharRNN(n_letters, n_hidden, len(alldata.labels_uniq))\n",
    "rnn.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb52aa3c-e633-4147-b0cc-abe55577e96c",
   "metadata": {},
   "source": [
    "## Computing the landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecf2c5b7-38af-4aa3-8c30-edd47b377728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subclass hessian to be consistent with our dataloader\n",
    "from pyhessian.hessian import PyHessian\n",
    "#from pyhessian.utils import get_params_grad\n",
    "class RNNHessian(PyHessian):\n",
    "    def __init__(self, model, criterion, data=None, dataloader=None, cuda=True):\n",
    "        \"\"\"\n",
    "        model: the model that needs Hessain information\n",
    "        criterion: the loss function\n",
    "        data: a single batch of data, including inputs and its corresponding labels\n",
    "        dataloader: the data loader including bunch of batches of data\n",
    "        \"\"\"\n",
    "        super().__init__(model, criterion, data, dataloader, cuda)\n",
    "        \n",
    "        self.inputs, self.targets = self.data\n",
    "        self.model.train()\n",
    "        # just guessing but I'm trying to force it to not use cudnn\n",
    "        rnn.eval()\n",
    "        rnn.rnn.train()\n",
    "        with torch.backends.cudnn.flags(enabled=False):\n",
    "            for i in zip(self.inputs, self.targets):\n",
    "                d, t = i\n",
    "                outputs = self.model(d)\n",
    "                loss = self.criterion(outputs, t)\n",
    "                loss.backward(create_graph=True)\n",
    "        \n",
    "        params, gradsH = get_params_grad(self.model)\n",
    "        self.params = params\n",
    "        self.gradsH = gradsH  # gradient used for Hessian computation\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b109fe9e-ee7e-451b-b4d0-cf7f592c6fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(label_tensor, data_tensor, data_label, data_item) = alldata[0:31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7f3f085-ea22-4d08-b451-405f72db108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a hessin calculator based on our model\n",
    "hessian_comp = RNNHessian(rnn, nn.NLLLoss(), data = (data_tensor, label_tensor), cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40d605c7-eb01-45ce-aafa-4de1a1c36acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfn = nn.NLLLoss()\n",
    "def loss_function(model, data):\n",
    "    dt, lt = data\n",
    "    batch_loss = 0\n",
    "    for d in zip(dt, lt): \n",
    "        tt, lbl_t = d\n",
    "        output = rnn.forward(tt)\n",
    "        loss = lfn(output, lbl_t)\n",
    "        batch_loss += loss\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c165ee7-79b1-4163-a980-c4135afb942c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing 3D loss landscape...\n",
      "Top 10 eigenvalues: [-8893.9951171875, -6956.69873046875, -5451.8564453125, -4937.12890625, 4483.798828125, -4347.18896484375, -4125.95849609375, -3888.14697265625, 3713.197265625, -2850.531982421875]\n",
      "Computing 68921 points in 3D space...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing 3D landscape: 100%|█████████████████████| 6893/6893 [18:17<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss hypercube stats - min: 0.0003680576919578016, max: 0.0686572715640068, mean: 0.010116196026461279\n"
     ]
    }
   ],
   "source": [
    "from pylosslandscapes.landscape import LossLandscape\n",
    "rnn.eval()\n",
    "rnn.rnn.train()\n",
    "\n",
    "with torch.backends.cudnn.flags(enabled=False):\n",
    "    landscape = LossLandscape.compute(\n",
    "    rnn,\n",
    "    (data_tensor, label_tensor),\n",
    "    device,\n",
    "    hessian_comp,\n",
    "    loss_function, # loss function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b499c3b-0189-466f-9b37-52cc49da415e",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 35.4 GiB for an array with shape (68921, 68920) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mlandscape\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow_profile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/Python/pylosslandscapes/src/pylosslandscapes/landscape.py:103\u001b[39m, in \u001b[36mshow_profile\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_persistence\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    102\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns the persistence of the landscape as a dictionary.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m get_persistence_dict(\u001b[38;5;28mself\u001b[39m.get_ms_complex())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/Python/pylosslandscapes/src/pylosslandscapes/landscape.py:94\u001b[39m, in \u001b[36mget_ms_complex\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_ms_complex\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ms_complex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m         ms_complex = tp.MorseSmaleComplex(\n\u001b[32m     95\u001b[39m             graph=\u001b[38;5;28mself\u001b[39m.graph, gradient=\u001b[33m\"\u001b[39m\u001b[33msteepest\u001b[39m\u001b[33m\"\u001b[39m, normalization=\u001b[33m\"\u001b[39m\u001b[33mfeature\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m         )\n\u001b[32m     97\u001b[39m         ms_complex.build(np.array(\u001b[38;5;28mself\u001b[39m.coords), \u001b[38;5;28mself\u001b[39m.loss.flatten())\n\u001b[32m     98\u001b[39m         \u001b[38;5;28mself\u001b[39m.ms_complex = ms_complex\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/Python/pylosslandscapes/.venv/lib/python3.13/site-packages/topopy/MorseSmaleComplex.py:109\u001b[39m, in \u001b[36mMorseSmaleComplex.build\u001b[39m\u001b[34m(self, X, Y, w)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y, w=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     87\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Assigns data to this object and builds the Morse-Smale Complex\u001b[39;00m\n\u001b[32m     88\u001b[39m \n\u001b[32m     89\u001b[39m \u001b[33;03m    Uses an internal graph given in the constructor to build a Morse-Smale\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \n\u001b[32m    108\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mMorseSmaleComplex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.debug:\n\u001b[32m    112\u001b[39m         sys.stdout.write(\u001b[33m\"\u001b[39m\u001b[33mDecomposition: \u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/Python/pylosslandscapes/.venv/lib/python3.13/site-packages/topopy/TopologicalObject.py:232\u001b[39m, in \u001b[36mTopologicalObject.build\u001b[39m\u001b[34m(self, X, Y, w)\u001b[39m\n\u001b[32m    229\u001b[39m     sys.stdout.write(\u001b[33m\"\u001b[39m\u001b[33mGraph Preparation: \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    230\u001b[39m     start = time.perf_counter()\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mXnorm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.debug:\n\u001b[32m    235\u001b[39m     end = time.perf_counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/Python/pylosslandscapes/.venv/lib/python3.13/site-packages/nglpy/EmptyRegionGraph.py:109\u001b[39m, in \u001b[36mEmptyRegionGraph.build\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    107\u001b[39m knn = sklearn.neighbors.NearestNeighbors(n_neighbors=\u001b[38;5;28mself\u001b[39m.max_neighbors)\n\u001b[32m    108\u001b[39m knn.fit(X)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m edges = \u001b[43mknn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# use pairs to prevent duplicates\u001b[39;00m\n\u001b[32m    112\u001b[39m pairs = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/Python/pylosslandscapes/.venv/lib/python3.13/site-packages/sklearn/neighbors/_base.py:869\u001b[39m, in \u001b[36mKNeighborsMixin.kneighbors\u001b[39m\u001b[34m(self, X, n_neighbors, return_distance)\u001b[39m\n\u001b[32m    862\u001b[39m use_pairwise_distances_reductions = (\n\u001b[32m    863\u001b[39m     \u001b[38;5;28mself\u001b[39m._fit_method == \u001b[33m\"\u001b[39m\u001b[33mbrute\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    864\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ArgKmin.is_usable_for(\n\u001b[32m    865\u001b[39m         X \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_X, \u001b[38;5;28mself\u001b[39m._fit_X, \u001b[38;5;28mself\u001b[39m.effective_metric_\n\u001b[32m    866\u001b[39m     )\n\u001b[32m    867\u001b[39m )\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m     results = \u001b[43mArgKmin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meffective_metric_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meffective_metric_params_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    880\u001b[39m     \u001b[38;5;28mself\u001b[39m._fit_method == \u001b[33m\"\u001b[39m\u001b[33mbrute\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metric == \u001b[33m\"\u001b[39m\u001b[33mprecomputed\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X)\n\u001b[32m    881\u001b[39m ):\n\u001b[32m    882\u001b[39m     results = _kneighbors_from_graph(\n\u001b[32m    883\u001b[39m         X, n_neighbors=n_neighbors, return_distance=return_distance\n\u001b[32m    884\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/Python/pylosslandscapes/.venv/lib/python3.13/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:281\u001b[39m, in \u001b[36mArgKmin.compute\u001b[39m\u001b[34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute the argkmin reduction.\u001b[39;00m\n\u001b[32m    201\u001b[39m \n\u001b[32m    202\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    278\u001b[39m \u001b[33;03mreturns.\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X.dtype == Y.dtype == np.float64:\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mArgKmin64\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m=\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X.dtype == Y.dtype == np.float32:\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin32.compute(\n\u001b[32m    294\u001b[39m         X=X,\n\u001b[32m    295\u001b[39m         Y=Y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    301\u001b[39m         return_distance=return_distance,\n\u001b[32m    302\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32msklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx:59\u001b[39m, in \u001b[36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx:77\u001b[39m, in \u001b[36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx:338\u001b[39m, in \u001b[36msklearn.metrics._pairwise_distances_reduction._argkmin.EuclideanArgKmin64.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx:132\u001b[39m, in \u001b[36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/Python/pylosslandscapes/.venv/lib/python3.13/site-packages/numpy/core/numeric.py:329\u001b[39m, in \u001b[36mfull\u001b[39m\u001b[34m(shape, fill_value, dtype, order, like)\u001b[39m\n\u001b[32m    327\u001b[39m     fill_value = asarray(fill_value)\n\u001b[32m    328\u001b[39m     dtype = fill_value.dtype\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m a = \u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m multiarray.copyto(a, fill_value, casting=\u001b[33m'\u001b[39m\u001b[33munsafe\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 35.4 GiB for an array with shape (68921, 68920) and data type int64"
     ]
    }
   ],
   "source": [
    "landscape.show_profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a6178e9a-c60f-4dac-8273-b2adce842729",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 35.4 GiB for an array with shape (68921, 68920) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m msc = \u001b[43mlandscape\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_ms_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/Python/pylosslandscapes/src/pylosslandscapes/landscape.py:94\u001b[39m, in \u001b[36mget_ms_complex\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_ms_complex\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ms_complex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m         ms_complex = tp.MorseSmaleComplex(\n\u001b[32m     95\u001b[39m             graph=\u001b[38;5;28mself\u001b[39m.graph, gradient=\u001b[33m\"\u001b[39m\u001b[33msteepest\u001b[39m\u001b[33m\"\u001b[39m, normalization=\u001b[33m\"\u001b[39m\u001b[33mfeature\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m         )\n\u001b[32m     97\u001b[39m         ms_complex.build(np.array(\u001b[38;5;28mself\u001b[39m.coords), \u001b[38;5;28mself\u001b[39m.loss.flatten())\n\u001b[32m     98\u001b[39m         \u001b[38;5;28mself\u001b[39m.ms_complex = ms_complex\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/Python/pylosslandscapes/.venv/lib/python3.13/site-packages/topopy/MorseSmaleComplex.py:109\u001b[39m, in \u001b[36mMorseSmaleComplex.build\u001b[39m\u001b[34m(self, X, Y, w)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y, w=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     87\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Assigns data to this object and builds the Morse-Smale Complex\u001b[39;00m\n\u001b[32m     88\u001b[39m \n\u001b[32m     89\u001b[39m \u001b[33;03m    Uses an internal graph given in the constructor to build a Morse-Smale\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \n\u001b[32m    108\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mMorseSmaleComplex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.debug:\n\u001b[32m    112\u001b[39m         sys.stdout.write(\u001b[33m\"\u001b[39m\u001b[33mDecomposition: \u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/Python/pylosslandscapes/.venv/lib/python3.13/site-packages/topopy/TopologicalObject.py:232\u001b[39m, in \u001b[36mTopologicalObject.build\u001b[39m\u001b[34m(self, X, Y, w)\u001b[39m\n\u001b[32m    229\u001b[39m     sys.stdout.write(\u001b[33m\"\u001b[39m\u001b[33mGraph Preparation: \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    230\u001b[39m     start = time.perf_counter()\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mXnorm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.debug:\n\u001b[32m    235\u001b[39m     end = time.perf_counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/Python/pylosslandscapes/.venv/lib/python3.13/site-packages/nglpy/EmptyRegionGraph.py:109\u001b[39m, in \u001b[36mEmptyRegionGraph.build\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    107\u001b[39m knn = sklearn.neighbors.NearestNeighbors(n_neighbors=\u001b[38;5;28mself\u001b[39m.max_neighbors)\n\u001b[32m    108\u001b[39m knn.fit(X)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m edges = \u001b[43mknn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# use pairs to prevent duplicates\u001b[39;00m\n\u001b[32m    112\u001b[39m pairs = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/Python/pylosslandscapes/.venv/lib/python3.13/site-packages/sklearn/neighbors/_base.py:869\u001b[39m, in \u001b[36mKNeighborsMixin.kneighbors\u001b[39m\u001b[34m(self, X, n_neighbors, return_distance)\u001b[39m\n\u001b[32m    862\u001b[39m use_pairwise_distances_reductions = (\n\u001b[32m    863\u001b[39m     \u001b[38;5;28mself\u001b[39m._fit_method == \u001b[33m\"\u001b[39m\u001b[33mbrute\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    864\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ArgKmin.is_usable_for(\n\u001b[32m    865\u001b[39m         X \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_X, \u001b[38;5;28mself\u001b[39m._fit_X, \u001b[38;5;28mself\u001b[39m.effective_metric_\n\u001b[32m    866\u001b[39m     )\n\u001b[32m    867\u001b[39m )\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m     results = \u001b[43mArgKmin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meffective_metric_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meffective_metric_params_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    880\u001b[39m     \u001b[38;5;28mself\u001b[39m._fit_method == \u001b[33m\"\u001b[39m\u001b[33mbrute\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metric == \u001b[33m\"\u001b[39m\u001b[33mprecomputed\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X)\n\u001b[32m    881\u001b[39m ):\n\u001b[32m    882\u001b[39m     results = _kneighbors_from_graph(\n\u001b[32m    883\u001b[39m         X, n_neighbors=n_neighbors, return_distance=return_distance\n\u001b[32m    884\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/Python/pylosslandscapes/.venv/lib/python3.13/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:281\u001b[39m, in \u001b[36mArgKmin.compute\u001b[39m\u001b[34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute the argkmin reduction.\u001b[39;00m\n\u001b[32m    201\u001b[39m \n\u001b[32m    202\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    278\u001b[39m \u001b[33;03mreturns.\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X.dtype == Y.dtype == np.float64:\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mArgKmin64\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m=\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X.dtype == Y.dtype == np.float32:\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin32.compute(\n\u001b[32m    294\u001b[39m         X=X,\n\u001b[32m    295\u001b[39m         Y=Y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    301\u001b[39m         return_distance=return_distance,\n\u001b[32m    302\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32msklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx:59\u001b[39m, in \u001b[36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx:77\u001b[39m, in \u001b[36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx:338\u001b[39m, in \u001b[36msklearn.metrics._pairwise_distances_reduction._argkmin.EuclideanArgKmin64.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx:132\u001b[39m, in \u001b[36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/Python/pylosslandscapes/.venv/lib/python3.13/site-packages/numpy/core/numeric.py:329\u001b[39m, in \u001b[36mfull\u001b[39m\u001b[34m(shape, fill_value, dtype, order, like)\u001b[39m\n\u001b[32m    327\u001b[39m     fill_value = asarray(fill_value)\n\u001b[32m    328\u001b[39m     dtype = fill_value.dtype\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m a = \u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m multiarray.copyto(a, fill_value, casting=\u001b[33m'\u001b[39m\u001b[33munsafe\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 35.4 GiB for an array with shape (68921, 68920) and data type int64"
     ]
    }
   ],
   "source": [
    "msc = landscape.get_ms_complex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a1c1d4-d8a3-49f9-b585-bf6683cd82e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
